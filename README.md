Neural Network from Scratch
This project implements a fully customizable feedforward neural network in Python, built from scratch using NumPy. It supports multiple layers, various activation functions, and the Adam optimizer, making it a flexible tool for experimenting with neural networks.

Features
Customizable Architecture: Define the number of layers and neurons per layer.
Activation Functions: Supports ReLU, Sigmoid, and Tanh.
Optimizers: Implements Adam optimization with bias correction.
Early Stopping: Stops training if the loss does not improve for a specified number of epochs.
Loss Function: Binary Cross-Entropy (BCE) for classification tasks.
Save and Load Weights: Save trained weights to a file and reload them later.
Visualization: Plots training loss over epochs.
